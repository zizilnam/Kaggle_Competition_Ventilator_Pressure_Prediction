{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_50features_smoothl1",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6bC0aylAuEe0ulfyw09i8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zizilnam/Kaggle_Ventilator_Pressure_Prediction/blob/main/baseline_50features_smoothl1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe7nuW6FiUut",
        "outputId": "e1e5d771-91c6-44b0-a000-3c66b0b64b9c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfnN8QCuiytu",
        "outputId": "a7558383-ff2a-4fd3-abd4-df45c58f8065"
      },
      "source": [
        "%cd /content/drive/MyDrive/Ventilator Pressure Prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Ventilator Pressure Prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ayE7q4W0UM5"
      },
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7VhsZ7riOqu",
        "outputId": "3b451e2d-3ad3-43da-da7f-e569f480259a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "import argparse\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "debug = False\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nK9OEhj9qlCG",
        "outputId": "8b6a83d7-a97e-4b55-d698-71b941c36fa9"
      },
      "source": [
        "pd.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.1.5'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv9_brlBiiU1"
      },
      "source": [
        "def create_features(df):\n",
        "    df = df.copy()\n",
        "    df['area'] = df['time_step'] * df['u_in']\n",
        "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
        "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
        "    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n",
        "    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n",
        "    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n",
        "    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n",
        "    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n",
        "    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n",
        "    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n",
        "    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n",
        "    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n",
        "    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n",
        "    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n",
        "    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n",
        "    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n",
        "    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n",
        "    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n",
        "    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n",
        "    df = df.fillna(0)\n",
        "    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n",
        "    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n",
        "    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
        "    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n",
        "    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n",
        "    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
        "    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
        "    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n",
        "    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
        "    df['cross']= df['u_in']*df['u_out']\n",
        "    df['cross2']= df['time_step']*df['u_out']\n",
        "\n",
        "    df['R'] = df['R'].astype(str)\n",
        "    df['C'] = df['C'].astype(str)\n",
        "    df = pd.get_dummies(df)\n",
        "\n",
        "    g = df.groupby('breath_id')['u_in'].apply(pd.Series)\n",
        "    \n",
        "    df['ewm_u_in_mean'] = g.ewm(halflife=10).mean()\\\n",
        "                           .reset_index(level=0, drop=True)\n",
        "    df['ewm_u_in_std'] = g.ewm(halflife=10).std()\\\n",
        "                          .reset_index(level=0, drop=True)\n",
        "    df['ewm_u_in_corr'] = g.ewm(halflife=10).corr()\\\n",
        "                           .reset_index(level=0, drop=True)\n",
        "\n",
        "    df['rolling_10_mean'] = g.rolling(window=10, min_periods=1).mean()\\\n",
        "                             .reset_index(level=0, drop=True)\n",
        "    df['rolling_10_max'] = g.rolling(window=10, min_periods=1).max()\\\n",
        "                            .reset_index(level=0, drop=True)\n",
        "    df['rolling_10_std'] = g.rolling(window=10, min_periods=1).std()\\\n",
        "                            .reset_index(level=0, drop=True)\n",
        "\n",
        "    df['expand_mean'] = g.expanding(2).mean()\\\n",
        "                         .reset_index(level=0, drop=True)\n",
        "    df['expand_max'] = g.expanding(2).max()\\\n",
        "                        .reset_index(level=0, drop=True)\n",
        "    df['expand_std'] = g.expanding(2).std()\\\n",
        "                        .reset_index(level=0, drop=True)\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    df.drop(['id', 'breath_id'], axis=1, inplace=True)\n",
        "    if 'pressure' in df.columns:\n",
        "        df.drop('pressure', axis=1, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y, w):\n",
        "        if y is None:\n",
        "            y = np.zeros(len(X), dtype=np.float32)\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.float32)\n",
        "        self.w = w.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i], self.w[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmTlIXday7wY",
        "outputId": "09778a67-a134-4f36-9332-4ceb44091ece"
      },
      "source": [
        "n = 100*1024 if debug else None\n",
        "\n",
        "train = reduce_mem_usage(pd.read_csv('train.csv'))\n",
        "#test = pd.read_csv('test.csv', nrows=n)\n",
        "#submit = pd.read_csv('sample_submission.csv', nrows=n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mem. usage decreased to 97.86 Mb (73.4% reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MHImvhKv-7N",
        "outputId": "9ea0a105-be91-491f-e599-49f146be730f"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6036000, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-bC3wzjqGMR"
      },
      "source": [
        "test = pd.read_csv('test.csv')\n",
        "submit = pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAB8bo41iuXM"
      },
      "source": [
        "features = create_features(train)\n",
        "rs = sklearn.preprocessing.RobustScaler()\n",
        "features = rs.fit_transform(features)  # => np.ndarray\n",
        "\n",
        "X_all = features.reshape(-1, 80, features.shape[-1])\n",
        "y_all = train.pressure.values.reshape(-1, 80)\n",
        "w_all = 1 - train.u_out.values.reshape(-1, 80)  # weights for the score, but not used in this notebook\n",
        "\n",
        "input_size = X_all.shape[2]\n",
        "\n",
        "# print(len(X_all))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0s7QZy7jpNt"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        hidden = [400, 300, 200, 100]\n",
        "        super().__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden[0],\n",
        "                             batch_first=True, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(2 * hidden[0], hidden[1],\n",
        "                             batch_first=True, bidirectional=True)\n",
        "        self.lstm3 = nn.LSTM(2 * hidden[1], hidden[2],\n",
        "                             batch_first=True, bidirectional=True)\n",
        "        self.lstm4 = nn.LSTM(2 * hidden[2], hidden[3],\n",
        "                             batch_first=True, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(2 * hidden[3], 50)\n",
        "        self.selu = nn.SELU()\n",
        "        self.fc2 = nn.Linear(50, 1)\n",
        "        self._reinitialize()\n",
        "\n",
        "    def _reinitialize(self):\n",
        "        \"\"\"\n",
        "        Tensorflow/Keras-like initialization\n",
        "        \"\"\"\n",
        "        for name, p in self.named_parameters():\n",
        "            if 'lstm' in name:\n",
        "                if 'weight_ih' in name:\n",
        "                    nn.init.xavier_uniform_(p.data)\n",
        "                elif 'weight_hh' in name:\n",
        "                    nn.init.orthogonal_(p.data)\n",
        "                elif 'bias_ih' in name:\n",
        "                    p.data.fill_(0)\n",
        "                    # Set forget-gate bias to 1\n",
        "                    n = p.size(0)\n",
        "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
        "                elif 'bias_hh' in name:\n",
        "                    p.data.fill_(0)\n",
        "            elif 'fc' in name:\n",
        "                if 'weight' in name:\n",
        "                    nn.init.xavier_uniform_(p.data)\n",
        "                elif 'bias' in name:\n",
        "                    p.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x, _ = self.lstm3(x)\n",
        "        x, _ = self.lstm4(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.selu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDQahMdNkNn4",
        "outputId": "1b22946f-8c5a-4169-f41b-b403ecdc467b"
      },
      "source": [
        "model = Model(input_size)\n",
        "for name, p in model.named_parameters():\n",
        "    print('%-32s %s' % (name, tuple(p.shape)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lstm1.weight_ih_l0               (1600, 50)\n",
            "lstm1.weight_hh_l0               (1600, 400)\n",
            "lstm1.bias_ih_l0                 (1600,)\n",
            "lstm1.bias_hh_l0                 (1600,)\n",
            "lstm1.weight_ih_l0_reverse       (1600, 50)\n",
            "lstm1.weight_hh_l0_reverse       (1600, 400)\n",
            "lstm1.bias_ih_l0_reverse         (1600,)\n",
            "lstm1.bias_hh_l0_reverse         (1600,)\n",
            "lstm2.weight_ih_l0               (1200, 800)\n",
            "lstm2.weight_hh_l0               (1200, 300)\n",
            "lstm2.bias_ih_l0                 (1200,)\n",
            "lstm2.bias_hh_l0                 (1200,)\n",
            "lstm2.weight_ih_l0_reverse       (1200, 800)\n",
            "lstm2.weight_hh_l0_reverse       (1200, 300)\n",
            "lstm2.bias_ih_l0_reverse         (1200,)\n",
            "lstm2.bias_hh_l0_reverse         (1200,)\n",
            "lstm3.weight_ih_l0               (800, 600)\n",
            "lstm3.weight_hh_l0               (800, 200)\n",
            "lstm3.bias_ih_l0                 (800,)\n",
            "lstm3.bias_hh_l0                 (800,)\n",
            "lstm3.weight_ih_l0_reverse       (800, 600)\n",
            "lstm3.weight_hh_l0_reverse       (800, 200)\n",
            "lstm3.bias_ih_l0_reverse         (800,)\n",
            "lstm3.bias_hh_l0_reverse         (800,)\n",
            "lstm4.weight_ih_l0               (400, 400)\n",
            "lstm4.weight_hh_l0               (400, 100)\n",
            "lstm4.bias_ih_l0                 (400,)\n",
            "lstm4.bias_hh_l0                 (400,)\n",
            "lstm4.weight_ih_l0_reverse       (400, 400)\n",
            "lstm4.weight_hh_l0_reverse       (400, 100)\n",
            "lstm4.bias_ih_l0_reverse         (400,)\n",
            "lstm4.bias_hh_l0_reverse         (400,)\n",
            "fc1.weight                       (50, 200)\n",
            "fc1.bias                         (50,)\n",
            "fc2.weight                       (1, 50)\n",
            "fc2.bias                         (1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXDB4brCkQRR"
      },
      "source": [
        "criterion = torch.nn.SmoothL1Loss()\n",
        "\n",
        "def evaluate(model, loader_val):\n",
        "    tb = time.time()\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    loss_sum = 0\n",
        "    score_sum = 0\n",
        "    n_sum = 0\n",
        "    y_pred_all = []\n",
        "\n",
        "    for ibatch, (x, y, w) in enumerate(loader_val):\n",
        "        n = y.size(0)\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        w = w.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x).squeeze()\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        n_sum += n\n",
        "        loss_sum += n*loss.item()\n",
        "        \n",
        "        y_pred_all.append(y_pred.cpu().detach().numpy())\n",
        "\n",
        "    loss_val = loss_sum / n_sum\n",
        "\n",
        "    model.train(was_training)\n",
        "\n",
        "    d = {'loss': loss_val,\n",
        "         'time': time.time() - tb,\n",
        "         'y_pred': np.concatenate(y_pred_all, axis=0)}\n",
        "\n",
        "    return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L20qqc_H2mh1",
        "outputId": "f820bd56-32af-4e04-e330-914f636f9446"
      },
      "source": [
        "nfold = 5\n",
        "kfold = KFold(n_splits=nfold, shuffle=True, random_state=228)\n",
        "splits = list(kfold.split(X_all))\n",
        "epochs = 2 if debug else 300\n",
        "lr = 1e-4\n",
        "batch_size = 1024\n",
        "max_grad_norm = 1000\n",
        "log = {}\n",
        "\n",
        "for ifold, (idx_train, idx_val) in enumerate(kfold.split(X_all)):\n",
        "    print('Fold %d' % ifold)\n",
        "    tb = time.time()\n",
        "    model = Model(input_size)\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\n",
        "\n",
        "    X_train = X_all[idx_train]\n",
        "    y_train = y_all[idx_train]\n",
        "    w_train = w_all[idx_train]\n",
        "    X_val = X_all[idx_val]\n",
        "    y_val = y_all[idx_val]\n",
        "    w_val = w_all[idx_val]\n",
        "\n",
        "    dataset_train = Dataset(X_train, y_train, w_train)\n",
        "    dataset_val = Dataset(X_val, y_val, w_val)\n",
        "    loader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True,\n",
        "                         batch_size=batch_size, drop_last=True)\n",
        "    loader_val = torch.utils.data.DataLoader(dataset_val, shuffle=False,\n",
        "                         batch_size=batch_size, drop_last=False)\n",
        "\n",
        "    losses_train = []\n",
        "    losses_val = []\n",
        "    lrs = []\n",
        "    time_val = 0\n",
        "    best_score = np.inf\n",
        "   \n",
        "    print('epoch loss_train loss_val lr time')\n",
        "    for iepoch in range(epochs):\n",
        "        loss_train = 0\n",
        "        n_sum = 0\n",
        "        \n",
        "        for ibatch, (x, y, w) in enumerate(loader_train):\n",
        "            n = y.size(0)\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred = model(x).squeeze()\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss_train += n*loss.item()\n",
        "            n_sum += n\n",
        "\n",
        "            loss.backward()\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        val = evaluate(model, loader_val)\n",
        "        loss_val = val['loss']\n",
        "        time_val += val['time']\n",
        "\n",
        "        losses_train.append(loss_train / n_sum)\n",
        "        losses_val.append(val['loss'])\n",
        "        lrs.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        print('%3d %9.6f %9.6f %7.3e %7.1f %6.1f' %\n",
        "              (iepoch + 1,\n",
        "               losses_train[-1], losses_val[-1], \n",
        "               lrs[-1], time.time() - tb, time_val))\n",
        "\n",
        "        scheduler.step(losses_val[-1])\n",
        "\n",
        "\n",
        "    ofilename = 'model%d.pth' % ifold\n",
        "    torch.save(model.state_dict(), ofilename)\n",
        "    print(ofilename, 'written')\n",
        "\n",
        "    log['fold%d' % ifold] = {\n",
        "        'loss_train': np.array(losses_train),\n",
        "        'loss_val': np.array(losses_val),\n",
        "        'learning_rate': np.array(lrs),\n",
        "        'y_pred': val['y_pred'],\n",
        "        'idx': idx_val\n",
        "    }\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0\n",
            "epoch loss_train loss_val lr time\n",
            "  1  4.305787  2.397480 1.000e-04    41.9    2.3\n",
            "  2  1.704251  1.127542 1.000e-04    71.3    4.6\n",
            "  3  0.949668  0.801678 1.000e-04   100.9    7.0\n",
            "  4  0.730836  0.672480 1.000e-04   130.5    9.3\n",
            "  5  0.632616  0.618930 1.000e-04   160.0   11.6\n",
            "  6  0.572048  0.545240 1.000e-04   189.7   14.0\n",
            "  7  0.526367  0.495182 1.000e-04   219.3   16.3\n",
            "  8  0.486764  0.475031 1.000e-04   248.9   18.6\n",
            "  9  0.448228  0.421377 1.000e-04   278.5   20.9\n",
            " 10  0.419997  0.387805 1.000e-04   308.1   23.3\n",
            " 11  0.380548  0.398223 1.000e-04   337.7   25.6\n",
            " 12  0.348142  0.333955 1.000e-04   367.3   27.9\n",
            " 13  0.328580  0.315376 1.000e-04   397.0   30.3\n",
            " 14  0.303846  0.301881 1.000e-04   426.6   32.6\n",
            " 15  0.289259  0.284170 1.000e-04   456.2   35.0\n",
            " 16  0.279779  0.281610 1.000e-04   485.8   37.3\n",
            " 17  0.265530  0.260386 1.000e-04   515.4   39.6\n",
            " 18  0.260993  0.251184 1.000e-04   545.1   42.0\n",
            " 19  0.254575  0.270541 1.000e-04   574.7   44.3\n",
            " 20  0.248508  0.245565 1.000e-04   604.3   46.6\n",
            " 21  0.237975  0.239228 1.000e-04   633.9   48.9\n",
            " 22  0.235872  0.232558 1.000e-04   663.6   51.3\n",
            " 23  0.225743  0.225355 1.000e-04   693.2   53.6\n",
            " 24  0.221896  0.218653 1.000e-04   722.8   55.9\n",
            " 25  0.213385  0.213859 1.000e-04   752.5   58.3\n",
            " 26  0.207608  0.206697 1.000e-04   782.1   60.7\n",
            " 27  0.208981  0.207942 1.000e-04   811.7   63.0\n",
            " 28  0.200823  0.200082 1.000e-04   841.3   65.4\n",
            " 29  0.195919  0.206067 1.000e-04   870.9   67.8\n",
            " 30  0.193973  0.198192 1.000e-04   900.5   70.2\n",
            " 31  0.187337  0.191537 1.000e-04   930.1   72.6\n",
            " 32  0.190316  0.190888 1.000e-04   959.7   75.0\n",
            " 33  0.187550  0.185146 1.000e-04   989.3   77.4\n",
            " 34  0.180996  0.208098 1.000e-04  1019.0   79.7\n",
            " 35  0.177763  0.189104 1.000e-04  1048.6   82.1\n",
            " 36  0.176365  0.178303 1.000e-04  1078.2   84.5\n",
            " 37  0.176025  0.172630 1.000e-04  1107.8   86.9\n",
            " 38  0.170950  0.172548 1.000e-04  1137.4   89.2\n",
            " 39  0.168302  0.173036 1.000e-04  1167.1   91.5\n",
            " 40  0.171478  0.173383 1.000e-04  1196.7   93.9\n",
            " 41  0.166412  0.169154 1.000e-04  1226.3   96.2\n",
            " 42  0.165597  0.166232 1.000e-04  1256.0   98.5\n",
            " 43  0.163641  0.178306 1.000e-04  1285.6  100.8\n",
            " 44  0.159650  0.164148 1.000e-04  1315.2  103.2\n",
            " 45  0.158617  0.177104 1.000e-04  1344.8  105.5\n",
            " 46  0.157263  0.161979 1.000e-04  1374.4  107.8\n",
            " 47  0.153105  0.158747 1.000e-04  1404.0  110.1\n",
            " 48  0.154288  0.156878 1.000e-04  1433.6  112.5\n",
            " 49  0.151463  0.165528 1.000e-04  1463.3  114.8\n",
            " 50  0.157772  0.156121 1.000e-04  1492.9  117.1\n",
            " 51  0.150575  0.151766 1.000e-04  1522.5  119.4\n",
            " 52  0.147297  0.157262 1.000e-04  1552.1  121.8\n",
            " 53  0.146398  0.152307 1.000e-04  1581.7  124.1\n",
            " 54  0.143699  0.150420 1.000e-04  1611.4  126.4\n",
            " 55  0.145745  0.153932 1.000e-04  1641.0  128.7\n",
            " 56  0.145415  0.150692 1.000e-04  1670.6  131.1\n",
            " 57  0.139441  0.149876 1.000e-04  1700.3  133.4\n",
            " 58  0.142618  0.149431 1.000e-04  1729.9  135.7\n",
            " 59  0.139009  0.148145 1.000e-04  1759.6  138.0\n",
            " 60  0.139798  0.157331 1.000e-04  1789.2  140.4\n",
            " 61  0.135536  0.146680 1.000e-04  1818.8  142.7\n",
            " 62  0.138035  0.142134 1.000e-04  1848.5  145.0\n",
            " 63  0.134865  0.142449 1.000e-04  1878.1  147.4\n",
            " 64  0.133214  0.149765 1.000e-04  1907.7  149.7\n",
            " 65  0.135076  0.142533 1.000e-04  1937.4  152.0\n",
            " 66  0.130738  0.143085 1.000e-04  1967.1  154.3\n",
            " 67  0.138191  0.138682 1.000e-04  1996.7  156.6\n",
            " 68  0.127858  0.136106 1.000e-04  2026.3  159.0\n",
            " 69  0.127186  0.141806 1.000e-04  2055.9  161.3\n",
            " 70  0.131651  0.140633 1.000e-04  2085.5  163.6\n",
            " 71  0.129340  0.140706 1.000e-04  2115.2  166.0\n",
            " 72  0.129097  0.154345 1.000e-04  2144.8  168.3\n",
            " 73  0.125339  0.133077 1.000e-04  2174.4  170.6\n",
            " 74  0.123837  0.133921 1.000e-04  2204.0  172.9\n",
            " 75  0.122003  0.133934 1.000e-04  2233.6  175.3\n",
            " 76  0.125726  0.155057 1.000e-04  2263.2  177.6\n",
            " 77  0.124417  0.137760 1.000e-04  2292.8  179.9\n",
            " 78  0.125672  0.134330 1.000e-04  2322.5  182.2\n",
            " 79  0.122728  0.130582 1.000e-04  2352.1  184.6\n",
            " 80  0.116335  0.129531 1.000e-04  2381.7  186.9\n",
            " 81  0.116553  0.128253 1.000e-04  2411.3  189.2\n",
            " 82  0.117468  0.134450 1.000e-04  2440.9  191.5\n",
            " 83  0.116221  0.133297 1.000e-04  2470.5  193.8\n",
            " 84  0.117791  0.129430 1.000e-04  2500.1  196.2\n",
            " 85  0.120385  0.128213 1.000e-04  2529.7  198.5\n",
            " 86  0.117660  0.125172 1.000e-04  2559.3  200.8\n",
            " 87  0.113951  0.136913 1.000e-04  2588.8  203.1\n",
            " 88  0.114304  0.129754 1.000e-04  2618.5  205.5\n",
            " 89  0.113690  0.123027 1.000e-04  2648.0  207.8\n",
            " 90  0.114044  0.124482 1.000e-04  2677.5  210.2\n",
            " 91  0.112577  0.122082 1.000e-04  2707.1  212.5\n",
            " 92  0.109624  0.125334 1.000e-04  2736.7  214.8\n",
            " 93  0.110778  0.123774 1.000e-04  2766.2  217.1\n",
            " 94  0.109991  0.130742 1.000e-04  2795.8  219.5\n",
            " 95  0.110182  0.126898 1.000e-04  2825.3  221.8\n",
            " 96  0.110868  0.122518 1.000e-04  2854.9  224.1\n",
            " 97  0.108873  0.120648 1.000e-04  2884.5  226.4\n",
            " 98  0.107973  0.126826 1.000e-04  2914.1  228.8\n",
            " 99  0.106229  0.123046 1.000e-04  2943.7  231.1\n",
            "100  0.107546  0.130058 1.000e-04  2973.3  233.4\n",
            "101  0.105225  0.128012 1.000e-04  3002.9  235.7\n",
            "102  0.107001  0.117991 1.000e-04  3032.5  238.1\n",
            "103  0.107107  0.119911 1.000e-04  3062.1  240.4\n",
            "104  0.106206  0.122950 1.000e-04  3091.7  242.7\n",
            "105  0.103281  0.115844 1.000e-04  3121.3  245.0\n",
            "106  0.106074  0.120095 1.000e-04  3150.9  247.3\n",
            "107  0.103982  0.121812 1.000e-04  3180.5  249.7\n",
            "108  0.104841  0.119099 1.000e-04  3210.1  252.0\n",
            "109  0.102124  0.116696 1.000e-04  3239.8  254.3\n",
            "110  0.100683  0.117410 1.000e-04  3269.5  256.7\n",
            "111  0.102162  0.115139 1.000e-04  3299.0  259.0\n",
            "112  0.102236  0.115953 1.000e-04  3328.7  261.4\n",
            "113  0.099488  0.117027 1.000e-04  3358.2  263.7\n",
            "114  0.098221  0.112030 1.000e-04  3387.9  266.0\n",
            "115  0.099646  0.112678 1.000e-04  3417.6  268.4\n",
            "116  0.098832  0.116864 1.000e-04  3447.2  270.7\n",
            "117  0.096745  0.115698 1.000e-04  3476.9  273.0\n",
            "118  0.096611  0.111663 1.000e-04  3506.6  275.4\n",
            "119  0.098378  0.111788 1.000e-04  3536.2  277.7\n",
            "120  0.099218  0.122258 1.000e-04  3565.9  280.0\n",
            "121  0.096452  0.118525 1.000e-04  3595.5  282.3\n",
            "122  0.095514  0.114225 1.000e-04  3625.2  284.7\n",
            "123  0.099138  0.124061 1.000e-04  3654.8  287.0\n",
            "124  0.097957  0.113845 1.000e-04  3684.4  289.3\n",
            "125  0.094499  0.114600 1.000e-04  3714.0  291.6\n",
            "126  0.097765  0.109199 1.000e-04  3743.6  294.0\n",
            "127  0.092768  0.107708 1.000e-04  3773.2  296.3\n",
            "128  0.092376  0.107369 1.000e-04  3802.9  298.7\n",
            "129  0.093726  0.108987 1.000e-04  3832.5  301.0\n",
            "130  0.090468  0.108955 1.000e-04  3862.1  303.4\n",
            "131  0.092163  0.109062 1.000e-04  3891.7  305.8\n",
            "132  0.092369  0.107580 1.000e-04  3921.3  308.1\n",
            "133  0.096176  0.117887 1.000e-04  3950.8  310.5\n",
            "134  0.090897  0.105639 1.000e-04  3980.5  312.8\n",
            "135  0.090315  0.105798 1.000e-04  4010.1  315.1\n",
            "136  0.088136  0.106317 1.000e-04  4039.7  317.4\n",
            "137  0.089337  0.108633 1.000e-04  4069.2  319.8\n",
            "138  0.090415  0.104753 1.000e-04  4098.8  322.1\n",
            "139  0.087045  0.109984 1.000e-04  4128.5  324.4\n",
            "140  0.086143  0.102444 1.000e-04  4158.1  326.7\n",
            "141  0.087053  0.102766 1.000e-04  4187.7  329.1\n",
            "142  0.090162  0.111343 1.000e-04  4217.4  331.4\n",
            "143  0.086740  0.105756 1.000e-04  4247.0  333.7\n",
            "144  0.084403  0.102865 1.000e-04  4276.7  336.0\n",
            "145  0.084664  0.101456 1.000e-04  4306.3  338.4\n",
            "146  0.084920  0.102134 1.000e-04  4336.0  340.7\n",
            "147  0.086513  0.099615 1.000e-04  4365.7  343.1\n",
            "148  0.085556  0.102463 1.000e-04  4395.4  345.4\n",
            "149  0.083897  0.103019 1.000e-04  4425.1  347.7\n",
            "150  0.083905  0.102668 1.000e-04  4454.7  350.1\n",
            "151  0.081444  0.102350 1.000e-04  4484.4  352.4\n",
            "152  0.081558  0.101028 1.000e-04  4514.0  354.7\n",
            "153  0.080994  0.097615 1.000e-04  4543.7  357.1\n",
            "154  0.084716  0.103978 1.000e-04  4573.3  359.4\n",
            "155  0.082509  0.100251 1.000e-04  4603.0  361.7\n",
            "156  0.079958  0.096233 1.000e-04  4632.6  364.0\n",
            "157  0.078567  0.098675 1.000e-04  4662.2  366.4\n",
            "158  0.078957  0.097985 1.000e-04  4691.9  368.7\n",
            "159  0.079892  0.096334 1.000e-04  4721.6  371.0\n",
            "160  0.080595  0.097337 1.000e-04  4751.3  373.4\n",
            "161  0.081491  0.100227 1.000e-04  4780.9  375.7\n",
            "162  0.080475  0.099877 1.000e-04  4810.6  378.0\n",
            "163  0.081680  0.098031 1.000e-04  4840.2  380.4\n",
            "164  0.079629  0.099128 1.000e-04  4869.8  382.8\n",
            "165  0.074889  0.095313 1.000e-04  4899.5  385.2\n",
            "166  0.076386  0.094938 1.000e-04  4929.1  387.6\n",
            "167  0.076190  0.095580 1.000e-04  4958.7  390.0\n",
            "168  0.075674  0.097440 1.000e-04  4988.4  392.3\n",
            "169  0.079593  0.098372 1.000e-04  5018.0  394.7\n",
            "170  0.074806  0.092504 1.000e-04  5047.7  397.1\n",
            "171  0.073132  0.093299 1.000e-04  5077.3  399.5\n",
            "172  0.072691  0.097109 1.000e-04  5106.9  401.8\n",
            "173  0.074665  0.094609 1.000e-04  5136.6  404.1\n",
            "174  0.072401  0.094045 1.000e-04  5166.2  406.4\n",
            "175  0.076685  0.096279 1.000e-04  5195.8  408.7\n",
            "176  0.074755  0.094308 1.000e-04  5225.4  411.1\n",
            "177  0.076474  0.106338 1.000e-04  5255.1  413.5\n",
            "178  0.073720  0.098954 1.000e-04  5284.6  415.8\n",
            "179  0.072910  0.091457 1.000e-04  5314.2  418.2\n",
            "180  0.069148  0.091407 1.000e-04  5343.9  420.5\n",
            "181  0.072384  0.095770 1.000e-04  5373.4  422.8\n",
            "182  0.070434  0.090361 1.000e-04  5403.0  425.1\n",
            "183  0.069704  0.101101 1.000e-04  5432.6  427.4\n",
            "184  0.071334  0.091788 1.000e-04  5462.2  429.8\n",
            "185  0.069828  0.089982 1.000e-04  5491.8  432.1\n",
            "186  0.068942  0.089254 1.000e-04  5521.4  434.4\n",
            "187  0.068962  0.092513 1.000e-04  5551.0  436.7\n",
            "188  0.070030  0.090512 1.000e-04  5580.7  439.0\n",
            "189  0.069241  0.089751 1.000e-04  5610.3  441.4\n",
            "190  0.072462  0.092317 1.000e-04  5639.9  443.7\n",
            "191  0.067659  0.091759 1.000e-04  5669.6  446.0\n",
            "192  0.067202  0.093049 1.000e-04  5699.2  448.3\n",
            "193  0.066680  0.090104 1.000e-04  5728.8  450.7\n",
            "194  0.069848  0.096203 1.000e-04  5758.5  453.0\n",
            "195  0.067829  0.088769 1.000e-04  5788.1  455.3\n",
            "196  0.066901  0.091453 1.000e-04  5817.8  457.7\n",
            "197  0.066823  0.088716 1.000e-04  5847.4  460.0\n",
            "198  0.066602  0.090123 1.000e-04  5877.0  462.3\n",
            "199  0.065064  0.088693 1.000e-04  5906.7  464.6\n",
            "200  0.066837  0.087284 1.000e-04  5936.3  466.9\n",
            "201  0.066976  0.088782 1.000e-04  5965.9  469.3\n",
            "202  0.065111  0.087569 1.000e-04  5995.6  471.6\n",
            "203  0.064493  0.089501 1.000e-04  6025.2  473.9\n",
            "204  0.064950  0.090739 1.000e-04  6054.8  476.3\n",
            "205  0.064077  0.088902 1.000e-04  6084.5  478.6\n",
            "206  0.065291  0.087688 1.000e-04  6114.1  480.9\n",
            "207  0.066032  0.085923 1.000e-04  6143.7  483.3\n",
            "208  0.063929  0.086600 1.000e-04  6173.4  485.6\n",
            "209  0.063719  0.086191 1.000e-04  6203.0  487.9\n",
            "210  0.064105  0.088672 1.000e-04  6232.6  490.2\n",
            "211  0.065319  0.098531 1.000e-04  6262.3  492.6\n",
            "212  0.065173  0.086957 1.000e-04  6291.9  494.9\n",
            "213  0.064251  0.085453 1.000e-04  6321.6  497.2\n",
            "214  0.063232  0.095315 1.000e-04  6351.2  499.5\n",
            "215  0.062565  0.083463 1.000e-04  6380.8  501.9\n",
            "216  0.058957  0.083659 1.000e-04  6410.4  504.2\n",
            "217  0.060312  0.083977 1.000e-04  6440.0  506.5\n",
            "218  0.058761  0.085521 1.000e-04  6469.6  508.8\n",
            "219  0.060355  0.084149 1.000e-04  6499.2  511.2\n",
            "220  0.059171  0.082008 1.000e-04  6528.8  513.5\n",
            "221  0.058142  0.085371 1.000e-04  6558.4  515.8\n",
            "222  0.059845  0.082276 1.000e-04  6588.0  518.1\n",
            "223  0.059021  0.083756 1.000e-04  6617.6  520.5\n",
            "224  0.058378  0.083109 1.000e-04  6647.2  522.8\n",
            "225  0.061261  0.094249 1.000e-04  6676.9  525.1\n",
            "226  0.064360  0.083257 1.000e-04  6706.5  527.4\n",
            "227  0.058663  0.084967 1.000e-04  6736.1  529.8\n",
            "228  0.057590  0.081581 1.000e-04  6765.8  532.1\n",
            "229  0.060574  0.083193 1.000e-04  6795.4  534.4\n",
            "230  0.062235  0.086674 1.000e-04  6825.0  536.8\n",
            "231  0.061315  0.083410 1.000e-04  6854.6  539.1\n",
            "232  0.058662  0.082559 1.000e-04  6884.3  541.4\n",
            "233  0.057473  0.083039 1.000e-04  6913.9  543.7\n",
            "234  0.056802  0.081723 1.000e-04  6943.5  546.1\n",
            "235  0.056940  0.083160 1.000e-04  6973.2  548.4\n",
            "236  0.056187  0.079956 1.000e-04  7002.8  550.7\n",
            "237  0.058876  0.082778 1.000e-04  7032.5  553.1\n",
            "238  0.058770  0.080807 1.000e-04  7062.1  555.4\n",
            "239  0.058384  0.087189 1.000e-04  7091.8  557.7\n",
            "240  0.058312  0.080376 1.000e-04  7121.4  560.0\n",
            "241  0.058910  0.081298 1.000e-04  7151.1  562.4\n",
            "242  0.058644  0.083092 1.000e-04  7180.7  564.7\n",
            "243  0.057143  0.081737 1.000e-04  7210.4  567.0\n",
            "244  0.055266  0.081274 1.000e-04  7240.0  569.4\n",
            "245  0.054921  0.082004 1.000e-04  7269.7  571.7\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4c30c58b1425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mtime_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-34bdb2020ba1>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader_val)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mn_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0my_pred_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMrWjSg13EbU"
      },
      "source": [
        "print('Fold loss_train loss_val best loss_val')\n",
        "for ifold in range(5):\n",
        "    d = log['fold%d' % ifold]\n",
        "    print('%4d %9.6f %9.6f %9.6f' % (ifold, d['loss_train'][-1], d['loss_val'][-1], np.min(d['loss_val'])))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax2_g2af48WU"
      },
      "source": [
        "(0.076626 + 0.078748 + 0.074072 + 0.082031 + 0.080473) / 5\n",
        "(0.048805 + 0.045513 + 0.045858 + 0.051170 + 0.049724) / 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr5zcneIroNQ"
      },
      "source": [
        "features = create_features(test)\n",
        "features = rs.fit_transform(features)\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcobIO-j1Rt8"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHmjnX46uk0x"
      },
      "source": [
        "features = create_features(test)\n",
        "features = rs.fit_transform(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcetL3E8_L3a"
      },
      "source": [
        "batch_size = 1024\n",
        " \n",
        "\n",
        "X_test = features.reshape(-1, 80, features.shape[-1])\n",
        "y_test = np.zeros(len(features)).reshape(-1, 80)\n",
        "w_test = 1 - test.u_out.values.reshape(-1, 80)\n",
        "\n",
        "dataset_test = Dataset(X_test, y_test, w_test)\n",
        "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size)\n",
        "\n",
        "y_pred_folds = np.zeros((len(test), 5), dtype=np.float32)\n",
        "for ifold in range(5):\n",
        "    model = Model(input_size)\n",
        "    model.to(device)\n",
        "    filename = 'model%d.pth' % ifold\n",
        "    model.load_state_dict(torch.load(filename, map_location=device))\n",
        "    model.eval()\n",
        "    \n",
        "    y_preds = []\n",
        "    for x, y, _ in loader_test:\n",
        "        x = x.to(device)\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x).squeeze()\n",
        "\n",
        "        y_preds.append(y_pred.cpu().numpy())\n",
        "    \n",
        "    y_preds = np.concatenate(y_preds, axis=0)\n",
        "    y_pred_folds[:, ifold] = y_preds.flatten()\n",
        "\n",
        "submit.pressure = np.median(y_pred_folds, axis=1)\n",
        "submit.to_csv('submission.csv', index=False)\n",
        "print('submission.csv written')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHjkAmrQ2_pV"
      },
      "source": [
        "submit"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
